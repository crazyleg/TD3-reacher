{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TD3 Agent in Reacher enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from agent import TD3Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Placeholder code that creates and dynamically updates training progress. Run in manually, make sure yoru have ipywidgets installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8e750bdc42464d9b22557863215a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Episode') ; ax.set_ylabel('Mean Score')\n",
    "ax.set_xlim(0,500) ; ax.set_ylim(0,50)\n",
    "xs, ys, mean_ys = [0], [0], [0]\n",
    "ax.plot(xs, ys, 'red', label = 'episode score')\n",
    "ax.plot(xs, mean_ys, 'blue', label = 'mean of last 100')\n",
    "ax.legend()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run actuall training for an agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.34899999219924255 with mean of 0.34899999219924255\n",
      "1 0.6864999846555315 with mean of 0.5177499884273871\n",
      "2 1.4039999686181512 with mean of 0.8131666484909751\n",
      "3 1.585999964550131 with mean of 1.006374977505764\n",
      "4 1.2369999723508966 with mean of 1.0524999764747904\n",
      "5 0.8624999807216261 with mean of 1.0208333105159297\n",
      "6 0.7784999825991715 with mean of 0.9862142636706787\n",
      "7 0.9544999786652676 with mean of 0.9822499780450022\n",
      "8 0.7714999827556348 with mean of 0.9588333119017393\n",
      "9 1.5489999653771485 with mean of 1.0178499772492802\n",
      "10 1.6549999630078578 with mean of 1.0757727032273328\n",
      "11 1.7359999611973678 with mean of 1.1307916413915025\n",
      "12 2.8689999358728433 with mean of 1.264499971736221\n",
      "13 4.026999909989518 with mean of 1.461821395897171\n",
      "14 5.5494998759590075 with mean of 1.7343332945679601\n",
      "15 7.332499836105849 with mean of 2.084218703414078\n",
      "16 10.58749976335097 with mean of 2.5844117069397776\n",
      "17 15.798499646876003 with mean of 3.3185277036029013\n",
      "18 20.70649953717361 with mean of 4.2336841158960965\n",
      "19 24.5579994510859 with mean of 5.249899882655587\n",
      "20 29.31749934470286 with mean of 6.3959760475149805\n",
      "21 33.58099924940618 with mean of 7.631658920328217\n",
      "22 34.745499223377585 with mean of 8.810521542199927\n",
      "23 38.13349914765008 with mean of 10.032312275760352\n",
      "24 38.71749913459653 with mean of 11.179719750113799\n",
      "25 38.46949914013989 with mean of 12.229326649730186\n",
      "26 38.24549914514664 with mean of 13.192888594004872\n",
      "27 37.47199916243571 with mean of 14.059999685734542\n",
      "28 36.95899917390219 with mean of 14.849620357740324\n",
      "29 37.01049917275105 with mean of 15.588316318240683\n",
      "30 37.260499167162976 with mean of 16.287418990786563\n",
      "31 36.65449918070823 with mean of 16.923890246721612\n",
      "32 37.264999167062605 with mean of 17.540287486731945\n",
      "33 37.26549916705134 with mean of 18.12044077144722\n",
      "34 36.63199918121117 with mean of 18.64934244029762\n",
      "35 37.531499161105785 with mean of 19.1738467936534\n",
      "36 37.997499150689876 with mean of 19.68259415465439\n",
      "37 37.56949916025639 with mean of 20.1533021811176\n",
      "38 36.938499174360395 with mean of 20.583691847611004\n",
      "39 36.98999917320922 with mean of 20.993849530750957\n",
      "40 37.4934991619552 with mean of 21.39628000956082\n",
      "41 37.38499916438037 with mean of 21.776963798961283\n",
      "42 36.67199918031707 with mean of 22.12335997062072\n",
      "43 36.588499182183426 with mean of 22.452113134519873\n",
      "44 37.46399916261453 with mean of 22.785710601810862\n",
      "45 36.295999188721424 with mean of 23.07941252761327\n",
      "46 37.2799991667272 with mean of 23.381552668871013\n",
      "47 37.68649915764128 with mean of 23.679572387387058\n",
      "48 37.432499163318674 with mean of 23.960244362406073\n",
      "49 36.88049917565686 with mean of 24.218649458671088\n",
      "50 37.589999159798246 with mean of 24.48083278614417\n",
      "51 37.05449917176751 with mean of 24.72263406279077\n",
      "52 37.53099916111697 with mean of 24.964301328796928\n",
      "53 38.126999147795296 with mean of 25.208054992111713\n",
      "54 37.59199915975359 with mean of 25.433217613341565\n",
      "55 36.232999190129476 with mean of 25.626070855784203\n",
      "56 37.18099916894004 with mean of 25.828788896365882\n",
      "57 36.66199918054063 with mean of 26.015568384024064\n",
      "58 36.13949919221927 with mean of 26.187160431620594\n",
      "59 36.30399918854256 with mean of 26.35577441090263\n",
      "60 37.17199916914119 with mean of 26.533089570873752\n",
      "61 37.35349916508445 with mean of 26.707612306264245\n",
      "62 36.84299917649489 with mean of 26.868491462934575\n",
      "63 36.92549917465092 with mean of 27.025632208430146\n",
      "64 35.576499204803376 with mean of 27.15718400837435\n",
      "65 36.762999178283174 with mean of 27.30272666246388\n",
      "66 37.60199915952993 with mean of 27.456447147494714\n",
      "67 38.06599914915884 with mean of 27.6124699710486\n",
      "68 37.17149916915247 with mean of 27.751006626093584\n",
      "69 37.08499917108576 with mean of 27.884349376736328\n",
      "70 36.2294991902078 with mean of 28.00188669805283\n",
      "71 37.38449916439152 with mean of 28.132200760085308\n",
      "72 35.85449919858953 with mean of 28.2379856702018\n",
      "73 34.98199921809152 with mean of 28.329120988416527\n",
      "74 33.78449924485747 with mean of 28.401859365169074\n",
      "75 36.80699917729957 with mean of 28.512453310065528\n",
      "76 34.06949923848732 with mean of 28.58462273770737\n",
      "77 36.129499192442836 with mean of 28.681351923024494\n",
      "78 36.77999917790318 with mean of 28.78386644523815\n",
      "79 36.686999179981925 with mean of 28.88265560442245\n",
      "80 37.46749916253637 with mean of 28.98864132736213\n",
      "81 37.20649916837007 with mean of 29.08885910591101\n",
      "82 37.986499150935785 with mean of 29.196059588381186\n",
      "83 36.64599918089815 with mean of 29.28474934543496\n",
      "84 36.014999195002126 with mean of 29.363928755429868\n",
      "85 37.65149915842357 with mean of 29.460295853139097\n",
      "86 37.491499161999876 with mean of 29.55260853485014\n",
      "87 35.52649920592093 with mean of 29.6204936561123\n",
      "88 35.743999201059516 with mean of 29.689297089201595\n",
      "89 37.426999163441515 with mean of 29.77527155669315\n",
      "90 35.404499208647806 with mean of 29.837131201220124\n",
      "91 35.21299921292827 with mean of 29.895564549173475\n",
      "92 37.79949915511544 with mean of 29.980553093323387\n",
      "93 38.148499147314794 with mean of 30.067446136450958\n",
      "94 37.920999152399766 with mean of 30.15011511556621\n",
      "95 37.83749915426614 with mean of 30.230192032635994\n",
      "96 37.14899916965536 with mean of 30.30151994126506\n",
      "97 36.26399918943651 with mean of 30.3623615662464\n",
      "98 36.33399918787198 with mean of 30.422681138182014\n",
      "99 37.41149916378803 with mean of 30.492569318438072\n",
      "--- SOLVED IT in 100 EPISODES ---\n"
     ]
    }
   ],
   "source": [
    "agent = TD3Agent(env)\n",
    "results = []\n",
    "\n",
    "solved = False\n",
    "episode = 0\n",
    "\n",
    "while not solved:\n",
    "    agent.reset()\n",
    "    \n",
    "    while True:\n",
    "        agent.step()\n",
    "        if np.any(agent.done):\n",
    "            results.append(agent.total_score)\n",
    "            xs.append(episode)\n",
    "            ys.append(agent.total_score)\n",
    "            mean_ys.append(np.array(results[-100:]).mean())\n",
    "            ax.plot(xs, ys, 'red', label = 'episode score')\n",
    "            ax.plot(xs, mean_ys, 'blue', label = 'mean of last 100')\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            \n",
    "            print(f'{episode} {agent.total_score} with mean of {np.array(results[-100:]).mean()}')\n",
    "            episode += 1\n",
    "            if np.array(results[-100:]).mean() > 30 and len(results)>=100:\n",
    "                print(f'--- SOLVED IT in {episode} EPISODES ---')\n",
    "                solved = True\n",
    "            agent.reset()        \n",
    "            break\n",
    "#             agent.eval_episode()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.actor_target.state_dict(), 'actor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.critic_target.state_dict(), 'critic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
